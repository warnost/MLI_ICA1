{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='https://fonts.googleapis.com/css?family=Passion+One' rel='stylesheet' type='text/css'><style>div.attn { font-family: 'Helvetica Neue'; font-size: 30px; line-height: 40px; color: #FFFFFF; text-align: center; margin: 30px 0; border-width: 10px 0; border-style: solid; border-color: #5AAAAA; padding: 30px 0; background-color: #DDDDFF; }hr { border: 0; background-color: #ffffff; border-top: 1px solid black; }hr.major { border-top: 10px solid #5AAA5A; }hr.minor { border: none; background-color: #ffffff; border-top: 5px dotted #CC3333; }div.bubble { width: 65%; padding: 20px; background: #DDDDDD; border-radius: 15px; margin: 0 auto; font-style: italic; color: #f00; }em { color: #AAA; }div.c1{visibility:hidden;margin:0;height:0;}div.note{color:red;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Ebnable HTML/CSS \n",
    "from __future__ import print_function\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<link href='https://fonts.googleapis.com/css?family=Passion+One' rel='stylesheet' type='text/css'><style>div.attn { font-family: 'Helvetica Neue'; font-size: 30px; line-height: 40px; color: #FFFFFF; text-align: center; margin: 30px 0; border-width: 10px 0; border-style: solid; border-color: #5AAAAA; padding: 30px 0; background-color: #DDDDFF; }hr { border: 0; background-color: #ffffff; border-top: 1px solid black; }hr.major { border-top: 10px solid #5AAA5A; }hr.minor { border: none; background-color: #ffffff; border-top: 5px dotted #CC3333; }div.bubble { width: 65%; padding: 20px; background: #DDDDDD; border-radius: 15px; margin: 0 auto; font-style: italic; color: #f00; }em { color: #AAA; }div.c1{visibility:hidden;margin:0;height:0;}div.note{color:red;}</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Enter Team Member Names here (double click to edit):\n",
    "\n",
    "- Name 1:\n",
    "- Name 2:\n",
    "- Name 3:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Class Assignment One\n",
    "In the following assignment you will be asked to fill in python code and derivations for a number of different problems. Please read all instructions carefully and turn in the rendered notebook (or HTML of the rendered notebook)  before the end of class (or right after class). The initial portion of this notebook is given before class and the remainder is given during class. Please answer the initial questions before class. Once class has started you may rework your answers as a team for the initial part of the assignment. \n",
    "\n",
    "<a id=\"top\"></a>\n",
    "## Contents\n",
    "* <a href=\"#Loading\">Loading the Data</a>\n",
    "* <a href=\"#linearnumpy\">Linear Regression</a>\n",
    "* <a href=\"#sklearn\">Using Scikit Learn for Regression</a>\n",
    "* <a href=\"#classification\">Linear Classification</a>\n",
    "\n",
    "________________________________________________________________________________________________________\n",
    "\n",
    "<a id=\"Loading\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "## Loading the Data\n",
    "Please run the following code to read in the \"diabetes\" dataset from sklearn's data loading module. \n",
    "\n",
    "This will load the data into the variable `ds`. `ds` is a dictionary object with fields like `ds.data`, which is a matrix of the continuous features in the dataset. The object is not a pandas dataframe. It is a numpy matrix. Each row is a set of observed instances, each column is a different feature. It also has a field called `ds.target` that is a continuous value we are trying to predict. Each entry in `ds.target` is a label for each row of the `ds.data` matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (442, 10) format is: ('rows', 'columns')\n",
      "range of target: 25.0 346.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "ds = load_diabetes()\n",
    "\n",
    "# this holds the continuous feature data\n",
    "# because ds.data is a matrix, there are some special properties we can access (like 'shape')\n",
    "print('features shape:', ds.data.shape, 'format is:', ('rows','columns')) # there are 442 instances and 10 features per instance\n",
    "print('range of target:', np.min(ds.target),np.max(ds.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
      "         0.01990842, -0.01764613],\n",
      "       [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
      "        -0.06832974, -0.09220405],\n",
      "       [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
      "         0.00286377, -0.02593034],\n",
      "       ...,\n",
      "       [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
      "        -0.04687948,  0.01549073],\n",
      "       [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
      "         0.04452837, -0.02593034],\n",
      "       [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
      "        -0.00421986,  0.00306441]])\n",
      "array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
      "        69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
      "        68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
      "        87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
      "       259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
      "       128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
      "       150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
      "       200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
      "        42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
      "        83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
      "       104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
      "       173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
      "       107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
      "        60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
      "       197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
      "        59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
      "       237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
      "       143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
      "       142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
      "        77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
      "        78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
      "       154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
      "        71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
      "       150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
      "       145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
      "        94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
      "        60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
      "        31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
      "       114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
      "       191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
      "       244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
      "       263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
      "        77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
      "        58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
      "       140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
      "       219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
      "        43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
      "       140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
      "        84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
      "        94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
      "       220.,  57.])\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# we can set the fields inside of ds and set them to new variables in python\n",
    "pprint(ds.data) # prints out elements of the matrix\n",
    "pprint(ds.target) # prints the vector (all 442 items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "<a id=\"linearnumpy\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "## Using Linear Regression \n",
    "In the videos, we derived the formula for calculating the optimal values of the regression weights (you must be connected to the internet for this equation to show up properly):\n",
    "\n",
    "$$ w = (X^TX)^{-1}X^Ty $$\n",
    "\n",
    "where $X$ is the matrix of values with a bias column of ones appended onto it. For the diabetes dataset one could construct this $X$ matrix by stacking a column of ones onto the `ds.data` matrix. \n",
    "\n",
    "$$ X=\\begin{bmatrix}\n",
    "         & \\vdots &        &  1 \\\\\n",
    "        \\dotsb & \\text{ds.data} & \\dotsb &  \\vdots\\\\\n",
    "         & \\vdots &         &  1\\\\\n",
    "     \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Question 1:** For the diabetes dataset, how many elements will the vector $w$ contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11 elements in w, one for each feature plus the one for the bias term.\n"
     ]
    }
   ],
   "source": [
    "# Enter your answer here (or write code to calculate it)\n",
    "\n",
    "# 11, one for each feature plus the one for the bias term.\n",
    "print('There are',ds.data.shape[1] + 1, 'elements in w, one for each feature plus the one for the bias term.')\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "\n",
    "**Exercise 1:** In the following empty cell, use this equation and numpy matrix operations to find the values of the vector $w$. You will need to be sure $X$ and $y$ are created like the instructor talked about in the video. Don't forget to include any modifications to $X$ to account for the bias term in $w$. You might be interested in the following functions:\n",
    "\n",
    "- `np.hstack((mat1,mat2))` stack two matrices horizontally, to create a new matrix\n",
    "- `np.ones((rows,cols))` create a matrix full of ones\n",
    "- `my_mat.T` takes transpose of numpy matrix named `my_mat`\n",
    "- `np.dot(mat1,mat2)` is matrix multiplication for two matrices\n",
    "- `np.linalg.inv(mat)` gets the inverse of the variable `mat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -10.01219782 -239.81908937  519.83978679  324.39042769 -792.18416163\n",
      "  476.74583782  101.04457032  177.06417623  751.27932109   67.62538639\n",
      "  152.13348416]\n"
     ]
    }
   ],
   "source": [
    "# Write you code here, print the values of the regression weights using the 'print()' function in python\n",
    "\n",
    "the_ones = np.ones((ds.data.shape[0],1))\n",
    "#print(the_ones.shape)\n",
    "\n",
    "X_wbias = np.hstack((ds.data,the_ones))\n",
    "#print(X_wbias.shape)\n",
    "\n",
    "dot_prod_inv = np.linalg.inv(np.dot(X_wbias.T, X_wbias))\n",
    "#print(dot_prod_inv.shape)\n",
    "\n",
    "weights = np.dot(np.dot(dot_prod_inv, X_wbias.T), ds.target)\n",
    "print(weights)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "___\n",
    "<a id=\"sklearn\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "# Start of Live Session Coding\n",
    "\n",
    "**Exercise 2:** Scikit-learn also has a linear regression fitting implementation. Look at the scikit learn API and learn to use the linear regression method. The API is here: \n",
    "\n",
    "- API Reference: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n",
    "Use the sklearn `LinearRegression` module to check your results from the previous question. \n",
    "\n",
    "**Question 2**: Did you get the same parameters? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model coefficients are: [ -10.01219782 -239.81908937  519.83978679  324.39042769 -792.18416163\n",
      "  476.74583782  101.04457032  177.06417623  751.27932109   67.62538639]\n",
      "model intercept is 152.1334841628965\n",
      "Answer to question is Yes\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# write your code here, print the values of model by accessing \n",
    "#    its properties that you looked up from the API\n",
    "\n",
    "reg = LinearRegression().fit(ds.data, ds.target)\n",
    "\n",
    "\n",
    "print('model coefficients are:', reg.coef_)\n",
    "print('model intercept is', reg.intercept_)\n",
    "print('Answer to question is', 'Yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "\n",
    "Recall that to predict the output from our model, $\\hat{y}$, from $w$ and $X$ we need to use the following formula:\n",
    "\n",
    "- $\\hat{y}=w^TX^T$\n",
    "\n",
    "Where $X$ is a matrix with example instances in *each row* of the matrix. \n",
    "\n",
    "**Exercise 3:** \n",
    "- *Part A:* Use matrix multiplication to predict output using numpy, $\\hat{y}_{numpy}$ and also using the sklearn regression object, $\\hat{y}_{sklearn}$.\n",
    " - **Note**: you may need to make the regression weights a column vector using the following code: `w = w.reshape((len(w),1))` This assumes your weights vector is assigned to the variable named `w`.\n",
    "- *Part B:* Calculate the mean squared error between your prediction from numpy and the target, $\\sum_i(y-\\hat{y}_{numpy})^2$. \n",
    "- *Part C:* Calculate the mean squared error between your sklearn prediction and the target, $\\sum_i(y-\\hat{y}_{sklearn})^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART A:\n",
      "SKLearn Predictions:\n",
      "[206.11706979  68.07234761 176.88406035 166.91796559 128.45984241\n",
      " 106.34908972  73.89417947 118.85378669 158.81033076 213.58408893\n",
      "  97.07853583  95.1016223  115.06673301 164.67605023 103.07517946\n",
      " 177.17236996 211.75953205 182.84424343 147.99987605 124.01702527\n",
      " 120.33094632  85.80377894 113.11286302 252.44934852 165.48821056\n",
      " 147.72187623  97.12824075 179.09342974 129.05497324 184.78138552\n",
      " 158.71515746  69.47588393 261.50255826 112.81897436  78.37194762\n",
      "  87.66624129 207.92460213 157.87686037 240.84370686 136.93372685\n",
      " 153.48187659  74.15703284 145.63105805  77.8280105  221.0786645\n",
      " 125.22224022 142.60147066 109.4926324   73.14037106 189.87368742\n",
      " 157.93636782 169.55816531 134.18186217 157.72356219 139.1077439\n",
      "  72.73252701 207.8289973   80.10834588 104.08562488 134.57807971\n",
      " 114.23779529 180.67760064  61.12644508  98.7215441  113.79626149\n",
      " 189.96141244 148.98263155 124.33457266 114.83969622 122.00224605\n",
      "  73.91315064 236.70948329 142.31366526 124.51427625 150.84273716\n",
      " 127.75408702 191.16674356  77.05921006 166.82129568  91.00741773\n",
      " 174.75026808 122.83488194  63.27214662 151.99895968  53.73407848\n",
      " 166.00134469  42.65030679 153.04135861  80.54493791 106.9048058\n",
      "  79.94239571 187.1634566  192.60115666  61.07125918 107.40466928\n",
      " 125.04038427 207.72180472 214.21749964 123.47505642 139.16396617\n",
      " 168.21035724 106.9267784  150.64502809 157.92231541 152.75856279\n",
      " 116.22255529  73.03090141 155.66898717 230.14278537 143.50191007\n",
      "  38.0947967  121.860737   152.79569851 207.99651918 291.23082717\n",
      " 189.17431487 214.02871163 235.18090808 165.3872774  151.25000032\n",
      " 156.57626783 200.44154589 219.35211772 174.79049427 169.23161767\n",
      " 187.8719893   57.49473392 108.55110499  92.68518048 210.87365701\n",
      " 245.47433558  69.84529943 113.0351432   68.42945176 141.69628649\n",
      " 239.46177949  58.3802079  235.47268158 254.91986281 253.31042713\n",
      " 155.50813249 230.55904185 170.44063216 117.99200943 178.55548636\n",
      " 240.07155813 190.3398776  228.66100769 114.24162642 178.36570405\n",
      " 209.09273631 144.85567253 200.65791056 121.34184881 150.50918174\n",
      " 199.02165018 146.2806806  124.02443772  85.26036769 235.16536625\n",
      "  82.17255475 231.29266191 144.36634395 197.04778326 146.99720377\n",
      "  77.18477545  59.3728572  262.67891084 225.12578458 220.20506312\n",
      "  46.59691745  88.1040833  221.77623752  97.24900614 164.48869956\n",
      " 119.90114263 157.79986195 223.08505437  99.5885471  165.84341641\n",
      " 179.47571002  89.83382843 171.82492808 158.36337775 201.47857482\n",
      " 186.39202728 197.47094269  66.57241937 154.59826802 116.18638034\n",
      " 195.92074021 128.04740268  91.20285628 140.56975398 155.23013996\n",
      " 169.70207476  98.75498537 190.1453107  142.5193942  177.26966106\n",
      "  95.31403505  69.0645889  164.16669511 198.06460718 178.26228169\n",
      " 228.58801706 160.67275473 212.28682319 222.48172067 172.85184399\n",
      " 125.27697688 174.7240982  152.38282657  98.58485669  99.73695497\n",
      " 262.29658755 223.73784832 221.3425256  133.61497308 145.42593933\n",
      "  53.04259372 141.81807792 153.68369915 125.21948824  77.25091512\n",
      " 230.26311068  78.90849563 105.20931175 117.99633487  99.06361032\n",
      " 166.55382825 159.34391027 158.27612808 143.05658763 231.55938678\n",
      " 176.64144413 187.23572317  65.38504165 190.66078824 179.74973878\n",
      " 234.91022512 119.15540438  85.63464409 100.85860205 140.4174259\n",
      " 101.83836332 120.66138775  83.06599161 234.58754656 245.16192142\n",
      " 263.26766492 274.87431887 180.67699732 203.05474761 254.21769367\n",
      " 118.44122343 268.44988948 104.83643442 115.87172349 140.45788952\n",
      "  58.46850453 129.83264097 263.78452618  45.01240356 123.28697604\n",
      " 131.08314499  34.89018315 138.35659686 244.30370588  89.95612306\n",
      " 192.07094588 164.32674962 147.74783541 191.89381753 176.44296313\n",
      " 158.34707354 189.19183226 116.58275843 111.44622859 117.45262547\n",
      " 165.79457547  97.80241129 139.54389024  84.17453643 159.9389204\n",
      " 202.4011919   80.48200416 146.64621068  79.05274311 191.33759392\n",
      " 220.67545196 203.75145711  92.87093594 179.15570241  81.80126162\n",
      " 152.82706623  76.79700486  97.79712384 106.83424483 123.83477117\n",
      " 218.13375502 126.02077447 206.76300555 230.57976636 122.0628518\n",
      " 135.67694517 126.36969016 148.49621551  88.07082258 138.95595037\n",
      " 203.86570118 172.55362727 122.95773416 213.92445645 174.88857841\n",
      " 110.07169487 198.36767241 173.24601643 162.64946177 193.31777358\n",
      " 191.53802295 284.13478714 279.30688474 216.0070265  210.08517801\n",
      " 216.22213925 157.01489819 224.06561179 189.05840605 103.56829281\n",
      " 178.70442926 111.81492124 290.99913121 182.64959461  79.33602602\n",
      "  86.33287509 249.15238929 174.51439576 122.10645431 146.27099383\n",
      " 170.6555544  183.50018707 163.36970989 157.03563376 144.42617093\n",
      " 125.30179325 177.50072942 104.57821235 132.1746674   95.06145678\n",
      " 249.9007786   86.24033937  62.00077469 156.81087903 192.3231713\n",
      " 133.85292727  93.67456315 202.49458467  52.53953733 174.82926235\n",
      " 196.9141296  118.06646574 235.3011088  165.09286707 160.41863314\n",
      " 162.37831419 254.05718804 257.23616403 197.50578991 184.06609359\n",
      "  58.62043851 194.3950396  110.77475548 142.20916765 128.82725506\n",
      " 180.12844365 211.26415225 169.59711427 164.34167693 136.2363478\n",
      " 174.50905908  74.67649224 246.29542114 114.14131338 111.54358708\n",
      " 140.02313284 109.99647408  91.37269237 163.01389345  75.16389857\n",
      " 254.05755095  53.47055785  98.48060512 100.66268306 258.58885744\n",
      " 170.67482041  61.91866052 182.3042492  171.26913027 189.19307553\n",
      " 187.18384852  87.12032949 148.37816611 251.35898288 199.69712357\n",
      " 283.63722409  50.85577124 172.14848891 204.06179478 174.16816194\n",
      " 157.93027543 150.50201654 232.9761832  121.5808709  164.54891787\n",
      " 172.67742636 226.78005938 149.46967223  99.14026374  80.43680779\n",
      " 140.15557121 191.90593837 199.27952034 153.63210613 171.80130949\n",
      " 112.11314588 162.60650576 129.8448476  258.02898298 100.70869427\n",
      " 115.87611124 122.53790409 218.17749233  60.94590955 131.09513588\n",
      " 119.48417359  52.60848094 193.01802803 101.05169913 121.22505534\n",
      " 211.8588945   53.44819015]\n",
      "Numpy Predictions:\n",
      "[[206.11706979  68.07234761 176.88406035 166.91796559 128.45984241\n",
      "  106.34908972  73.89417947 118.85378669 158.81033076 213.58408893\n",
      "   97.07853583  95.1016223  115.06673301 164.67605023 103.07517946\n",
      "  177.17236996 211.75953205 182.84424343 147.99987605 124.01702527\n",
      "  120.33094632  85.80377894 113.11286302 252.44934852 165.48821056\n",
      "  147.72187623  97.12824075 179.09342974 129.05497324 184.78138552\n",
      "  158.71515746  69.47588393 261.50255826 112.81897436  78.37194762\n",
      "   87.66624129 207.92460213 157.87686037 240.84370686 136.93372685\n",
      "  153.48187659  74.15703284 145.63105805  77.8280105  221.0786645\n",
      "  125.22224022 142.60147066 109.4926324   73.14037106 189.87368742\n",
      "  157.93636782 169.55816531 134.18186217 157.72356219 139.1077439\n",
      "   72.73252701 207.8289973   80.10834588 104.08562488 134.57807971\n",
      "  114.23779529 180.67760064  61.12644508  98.7215441  113.79626149\n",
      "  189.96141244 148.98263155 124.33457266 114.83969622 122.00224605\n",
      "   73.91315064 236.70948329 142.31366526 124.51427625 150.84273716\n",
      "  127.75408702 191.16674356  77.05921006 166.82129568  91.00741773\n",
      "  174.75026808 122.83488194  63.27214662 151.99895968  53.73407848\n",
      "  166.00134469  42.65030679 153.04135861  80.54493791 106.9048058\n",
      "   79.94239571 187.1634566  192.60115666  61.07125918 107.40466928\n",
      "  125.04038427 207.72180472 214.21749964 123.47505642 139.16396617\n",
      "  168.21035724 106.9267784  150.64502809 157.92231541 152.75856279\n",
      "  116.22255529  73.03090141 155.66898717 230.14278537 143.50191007\n",
      "   38.0947967  121.860737   152.79569851 207.99651918 291.23082717\n",
      "  189.17431487 214.02871163 235.18090808 165.3872774  151.25000032\n",
      "  156.57626783 200.44154589 219.35211772 174.79049427 169.23161767\n",
      "  187.8719893   57.49473392 108.55110499  92.68518048 210.87365701\n",
      "  245.47433558  69.84529943 113.0351432   68.42945176 141.69628649\n",
      "  239.46177949  58.3802079  235.47268158 254.91986281 253.31042713\n",
      "  155.50813249 230.55904185 170.44063216 117.99200943 178.55548636\n",
      "  240.07155813 190.3398776  228.66100769 114.24162642 178.36570405\n",
      "  209.09273631 144.85567253 200.65791056 121.34184881 150.50918174\n",
      "  199.02165018 146.2806806  124.02443772  85.26036769 235.16536625\n",
      "   82.17255475 231.29266191 144.36634395 197.04778326 146.99720377\n",
      "   77.18477545  59.3728572  262.67891084 225.12578458 220.20506312\n",
      "   46.59691745  88.1040833  221.77623752  97.24900614 164.48869956\n",
      "  119.90114263 157.79986195 223.08505437  99.5885471  165.84341641\n",
      "  179.47571002  89.83382843 171.82492808 158.36337775 201.47857482\n",
      "  186.39202728 197.47094269  66.57241937 154.59826802 116.18638034\n",
      "  195.92074021 128.04740268  91.20285628 140.56975398 155.23013996\n",
      "  169.70207476  98.75498537 190.1453107  142.5193942  177.26966106\n",
      "   95.31403505  69.0645889  164.16669511 198.06460718 178.26228169\n",
      "  228.58801706 160.67275473 212.28682319 222.48172067 172.85184399\n",
      "  125.27697688 174.7240982  152.38282657  98.58485669  99.73695497\n",
      "  262.29658755 223.73784832 221.3425256  133.61497308 145.42593933\n",
      "   53.04259372 141.81807792 153.68369915 125.21948824  77.25091512\n",
      "  230.26311068  78.90849563 105.20931175 117.99633487  99.06361032\n",
      "  166.55382825 159.34391027 158.27612808 143.05658763 231.55938678\n",
      "  176.64144413 187.23572317  65.38504165 190.66078824 179.74973878\n",
      "  234.91022512 119.15540438  85.63464409 100.85860205 140.4174259\n",
      "  101.83836332 120.66138775  83.06599161 234.58754656 245.16192142\n",
      "  263.26766492 274.87431887 180.67699732 203.05474761 254.21769367\n",
      "  118.44122343 268.44988948 104.83643442 115.87172349 140.45788952\n",
      "   58.46850453 129.83264097 263.78452618  45.01240356 123.28697604\n",
      "  131.08314499  34.89018315 138.35659686 244.30370588  89.95612306\n",
      "  192.07094588 164.32674962 147.74783541 191.89381753 176.44296313\n",
      "  158.34707354 189.19183226 116.58275843 111.44622859 117.45262547\n",
      "  165.79457547  97.80241129 139.54389024  84.17453643 159.9389204\n",
      "  202.4011919   80.48200416 146.64621068  79.05274311 191.33759392\n",
      "  220.67545196 203.75145711  92.87093594 179.15570241  81.80126162\n",
      "  152.82706623  76.79700486  97.79712384 106.83424483 123.83477117\n",
      "  218.13375502 126.02077447 206.76300555 230.57976636 122.0628518\n",
      "  135.67694517 126.36969016 148.49621551  88.07082258 138.95595037\n",
      "  203.86570118 172.55362727 122.95773416 213.92445645 174.88857841\n",
      "  110.07169487 198.36767241 173.24601643 162.64946177 193.31777358\n",
      "  191.53802295 284.13478714 279.30688474 216.0070265  210.08517801\n",
      "  216.22213925 157.01489819 224.06561179 189.05840605 103.56829281\n",
      "  178.70442926 111.81492124 290.99913121 182.64959461  79.33602602\n",
      "   86.33287509 249.15238929 174.51439576 122.10645431 146.27099383\n",
      "  170.6555544  183.50018707 163.36970989 157.03563376 144.42617093\n",
      "  125.30179325 177.50072942 104.57821235 132.1746674   95.06145678\n",
      "  249.9007786   86.24033937  62.00077469 156.81087903 192.3231713\n",
      "  133.85292727  93.67456315 202.49458467  52.53953733 174.82926235\n",
      "  196.9141296  118.06646574 235.3011088  165.09286707 160.41863314\n",
      "  162.37831419 254.05718804 257.23616403 197.50578991 184.06609359\n",
      "   58.62043851 194.3950396  110.77475548 142.20916765 128.82725506\n",
      "  180.12844365 211.26415225 169.59711427 164.34167693 136.2363478\n",
      "  174.50905908  74.67649224 246.29542114 114.14131338 111.54358708\n",
      "  140.02313284 109.99647408  91.37269237 163.01389345  75.16389857\n",
      "  254.05755095  53.47055785  98.48060512 100.66268306 258.58885744\n",
      "  170.67482041  61.91866052 182.3042492  171.26913027 189.19307553\n",
      "  187.18384852  87.12032949 148.37816611 251.35898288 199.69712357\n",
      "  283.63722409  50.85577124 172.14848891 204.06179478 174.16816194\n",
      "  157.93027543 150.50201654 232.9761832  121.5808709  164.54891787\n",
      "  172.67742636 226.78005938 149.46967223  99.14026374  80.43680779\n",
      "  140.15557121 191.90593837 199.27952034 153.63210613 171.80130949\n",
      "  112.11314588 162.60650576 129.8448476  258.02898298 100.70869427\n",
      "  115.87611124 122.53790409 218.17749233  60.94590955 131.09513588\n",
      "  119.48417359  52.60848094 193.01802803 101.05169913 121.22505534\n",
      "  211.8588945   53.44819015]]\n"
     ]
    }
   ],
   "source": [
    "# Use this block to answer the questions\n",
    "\n",
    "# w = w.reshape((len(w),1)) # make w a column vector\n",
    "w = reg.coef_\n",
    "w = w.reshape((len(w),1))\n",
    "yhat_np = np.dot(w.T, ds.data.T) + reg.intercept_\n",
    "yhat_skl = reg.predict(ds.data)\n",
    "print('PART A:')\n",
    "print('SKLearn Predictions:')\n",
    "print(yhat_skl)\n",
    "print('Numpy Predictions:')\n",
    "print(yhat_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART B:\n",
      "MSE Sklearn is: 2859.6903987680657\n"
     ]
    }
   ],
   "source": [
    "#PART B:\n",
    "MSE_np = mean_squared_error(ds.target, yhat_np.T)\n",
    "print('PART B:')\n",
    "print('MSE Sklearn is:', MSE_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART C:\n",
      "MSE Numpy is: 2859.6903987680657\n"
     ]
    }
   ],
   "source": [
    "#Part C:\n",
    "MSE_skl = mean_squared_error(ds.target, yhat_skl)\n",
    "print('PART C:')\n",
    "print('MSE Numpy is:', MSE_skl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "<a id=\"classification\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "## Using Linear Classification\n",
    "Now lets use the code you created to make a classifier with linear boundaries. Run the following code in order to load the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (150, 4)\n",
      "original number of classes: 3\n",
      "new number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "# this will overwrite the diabetes dataset\n",
    "ds = load_iris()\n",
    "print('features shape:', ds.data.shape) # there are 150 instances and 4 features per instance\n",
    "print('original number of classes:', len(np.unique(ds.target)))\n",
    "\n",
    "# now let's make this a binary classification task\n",
    "ds.target = ds.target>1\n",
    "print ('new number of classes:', len(np.unique(ds.target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "\n",
    "**Exercise 4:** Now use linear regression to come up with a set of weights, `w`, that predict the class value. This is exactly like you did before for the *diabetes* dataset. However, instead of regressing to continuous values, you are just regressing to the integer value of the class (0 or 1), like we talked about in the video. Remember to account for the bias term when constructing the feature matrix, `X`. Print the weights of the linear classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the weights of the linear classifier: [-0.04587608  0.20276839  0.00398791  0.55177932 -0.69528186]\n"
     ]
    }
   ],
   "source": [
    "# write your code here and print the values of the weights \n",
    "\n",
    "# use this box to predict the classification output\n",
    "the_ones = np.ones((ds.data.shape[0],1))\n",
    "#print(the_ones.shape)\n",
    "\n",
    "X_wbias = np.hstack((ds.data,the_ones))\n",
    "#print(X_wbias.shape)\n",
    "\n",
    "dot_prod_inv = np.linalg.inv(np.dot(X_wbias.T, X_wbias))\n",
    "#print(dot_prod_inv.shape)\n",
    "\n",
    "weights = np.dot(np.dot(dot_prod_inv, X_wbias.T), ds.target)\n",
    "print('These are the weights of the linear classifier:',weights)\n",
    "\n",
    "\n",
    "## Just making sure they are the same, output omitted for clarity\n",
    "reg = LinearRegression().fit(ds.data, ds.target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "\n",
    "**Exercise 5:** Finally, use a hard decision function on the output of the linear regression to make this a binary classifier. This is just like we talked about in the video, where the output of the linear regression passes through a function: \n",
    "\n",
    "- $\\hat{y}=g(w^TX^T)$ where\n",
    " - $g(w^TX^T)$ for $w^TX^T < \\alpha$ maps the predicted class to `0` \n",
    " - $g(w^TX^T)$ for $w^TX^T \\geq \\alpha$ maps the predicted class to `1`. \n",
    "\n",
    "Here, alpha is a threshold for deciding the class. \n",
    "\n",
    "**Question 3**: What value for $\\alpha$ makes the most sense? What is the accuracy of the classifier given the $\\alpha$ you chose? \n",
    "\n",
    "Note: You can calculate the accuracy with the following code: `accuracy = float(sum(yhat==y)) / len(y)` assuming you choose variable names `y` and `yhat` for the target and prediction, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def accuracy(alpha,yhat,y):\n",
    "    yhat = yhat > alpha\n",
    "    accuracy = float(sum(yhat==y)) / len(y)\n",
    "    return accuracy\n",
    "\n",
    "alphas = np.linspace(-0.2, 1.2, 100)\n",
    "\n",
    "mapped = []\n",
    "\n",
    "for i in alphas:\n",
    "    mapped.append({'alpha':i,'accuracy':accuracy(i, reg.predict(ds.data),ds.target)}) \n",
    "\n",
    "df = pd.DataFrame(mapped)\n",
    "\n",
    "plot = sns.lineplot(x=\"alpha\", y=\"accuracy\", data=df).set_title('Accuracy by value of Alpha')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph shows the accuracy scores over several different alphas. We can see it peaks between 0.4 and 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Alpha = 0.51\n",
      "Max Accuracy Percentage = 94.0\n"
     ]
    }
   ],
   "source": [
    "max_alpha = df.iloc[df.accuracy.idxmax()]\n",
    "print('Max Alpha =', round(max_alpha[1],2))\n",
    "print('Max Accuracy Percentage =', round(max_alpha[0]*100,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best alpha is 0.51 for an accuracy of 94%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "\n",
    "That's all! Please **save (make sure you saved!!!) and upload your rendered notebook** and please include **team member names** in the notebook submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
